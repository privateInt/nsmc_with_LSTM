{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261f5f24-d758-47d6-a89b-e30cf21bc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85581ae-877a-4ae1-811f-69cc8027acb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145393/145393 [05:36<00:00, 432.47it/s]\n",
      "100%|██████████| 48852/48852 [02:00<00:00, 404.55it/s]\n",
      "/home/ubuntu/anaconda3/envs/seunghoon/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 1.데이터 로드\n",
    "root_path = '/home/ubuntu/workspace/seunghoon/naver_movie_text_classification/LSTM/dataset/nsmc'\n",
    "\n",
    "train_data = pd.read_table(os.path.join(root_path, 'ratings_train.txt'))\n",
    "test_data = pd.read_table(os.path.join(root_path, 'ratings_test.txt'))\n",
    "\n",
    "# 2.데이터 정제\n",
    "train_data['document'].nunique(), train_data['label'].nunique() # 중복제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열의 중복 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\", regex=True) # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "test_data['document'].nunique(), test_data['label'].nunique() # 중복 제거\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거\n",
    "\n",
    "\n",
    "# 3.토큰화\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)\n",
    "\n",
    "# 4.정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "# 5.빈 샘플 제거\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "# 6.패딩\n",
    "max_len = 30\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7494595-56d1-4ce1-a1a6-d47cee2ed33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_lst = [train_data, test_data, tokenizer, X_train, X_test, y_train, y_test]\n",
    "name_lst = [\"train_data.pkl\", \"test_data.pkl\", \"tokenizer.pkl\", \"X_train.pkl\", \"X_test.pkl\", \"y_train.pkl\", \"y_test.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d439e8c8-a6e2-4ce5-b97f-594a0c01890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, object_name in zip(name_lst, pickle_lst):\n",
    "    with open(name, \"wb\")as f:\n",
    "        pickle.dump(object_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7852f15f-14a5-41cf-996e-2a7a4cdb7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/ubuntu/workspace/seunghoon/naver_movie_text_classification/LSTM/dataset/nsmc'\n",
    "\n",
    "train_data = pd.read_table(os.path.join(root_path, 'ratings_train.txt'))\n",
    "test_data = pd.read_table(os.path.join(root_path, 'ratings_test.txt'))\n",
    "\n",
    "# 2.데이터 정제\n",
    "train_data['document'].nunique(), train_data['label'].nunique() # 중복제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열의 중복 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\", regex=True) # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "test_data['document'].nunique(), test_data['label'].nunique() # 중복 제거\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\", regex=True) # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e2b3fd1-3982-4b8b-b763-58a0c923938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145393\n",
      "48852\n",
      "        id           document  label\n",
      "0  9976970  아 더빙 진짜 짜증나네요 목소리      0\n",
      "        id document  label\n",
      "0  6270596      굳 ㅋ      1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train_data[:1])\n",
    "print(test_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799b42bd-c11a-4c56-895c-d7f72aff024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(dataset):\n",
    "    dataset['document'].nunique(), dataset['label'].nunique() # 중복제거\n",
    "    dataset.drop_duplicates(subset=['document'], inplace=True) # document 열의 중복 제거\n",
    "    dataset['document'] = dataset['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True) # 한글과 공백을 제외하고 모두 제거\n",
    "    dataset['document'] = dataset['document'].str.replace('^ +', \"\", regex=True) # white space 데이터를 empty value로 변경\n",
    "    dataset['document'].replace('', np.nan, inplace=True)\n",
    "    dataset = dataset.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ec77c48-346b-4686-a17d-9ac6648001f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/ubuntu/workspace/seunghoon/naver_movie_text_classification/LSTM/dataset/nsmc'\n",
    "\n",
    "train_data = pd.read_table(os.path.join(root_path, 'ratings_train.txt'))\n",
    "test_data = pd.read_table(os.path.join(root_path, 'ratings_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27eabbc5-05ba-4a9d-a8fb-bfd42538a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleansing(train_data)\n",
    "data_cleansing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85eeffcf-fadb-4e74-8a60-21ac955dc8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146183\n",
      "49158\n",
      "        id           document  label\n",
      "0  9976970  아 더빙 진짜 짜증나네요 목소리      0\n",
      "        id document  label\n",
      "0  6270596      굳 ㅋ      1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(train_data[:1])\n",
    "print(test_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4e2ce6d-98e8-4686-8712-c6151dcd95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lst = [1,2,3]\n",
    "b_lst = ['a','b','c']\n",
    "c_lst = ['가','나','다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3082d94-825c-4eea-8bb2-7890c93641bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_lst = [a_lst, b_lst, c_lst]\n",
    "name_lst = [\"test1.pkl\", \"test2.pkl\", \"test3.pkl\"]\n",
    "\n",
    "for object_name, name in zip(object_lst, name_lst):\n",
    "    joblib.dump(object_name, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4bff3-c0ad-4a96-98f4-abbe39196fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91bdf7f3-bd03-4e68-84be-a585809d3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.74% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "rslt = requests.post('http://localhost:1234/home', json.dumps('ㅋㅋ 응 안봐~')).text\n",
    "print(json.loads(rslt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seunghoon",
   "language": "python",
   "name": "seunghoon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
